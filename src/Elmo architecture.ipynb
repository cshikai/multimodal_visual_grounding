{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "d9726989",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.dataset import VisualGroundingDataset\n",
    "from collections import OrderedDict\n",
    "import os\n",
    "import torch\n",
    "    import pytorch_lightning as pl\n",
    "    import torchvision.models as models\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "dd4a441a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elmo embedder initialized on gpu?: True\n"
     ]
    }
   ],
   "source": [
    "from config.config import cfg\n",
    "dataset = VisualGroundingDataset('train', cfg)\n",
    "img, text,length = dataset[1]\n",
    "text = text.unsqueeze(0).expand(3,-1,-1)\n",
    "length = [length,length,length]\n",
    "img = img.unsqueeze(0).expand(3,-1,-1,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "23ea3412",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisualFeatures(pl.LightningModule):\n",
    "\n",
    "# layer ids correspond to layers 4.1,4.3,5.1,5.3 in the vgg16 architecture\n",
    "    FEATURE_LAYERS_ID = ['18','22','25','29']\n",
    "    M = 18\n",
    "    D = 1024\n",
    "    NUM_CONV_LAYERS = 3\n",
    "    alpha = 0.25\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        image_model = models.vgg16(pretrained=False)\n",
    "        checkpoint = torch.load('/models/cache/hub/checkpoints/vgg16-397923af.pth')\n",
    "        image_model.load_state_dict(checkpoint)\n",
    "        self.pretrained_model = list(image_model.children())[0]\n",
    "        \n",
    "        for parameter in self.pretrained_model.parameters():\n",
    "            parameter.requires_grad = False\n",
    "        \n",
    "        self.raw_visual_features = OrderedDict()\n",
    "        self.visual_features = OrderedDict()\n",
    "        \n",
    "        #no explict need to reference these hooks ,but reference them for potential future use\n",
    "        self.forward_hooks = []\n",
    "        \n",
    "        \n",
    "        for l in list(self.pretrained_model._modules.keys()):\n",
    "            \n",
    "            if l in self.FEATURE_LAYERS_ID:\n",
    "                self.forward_hooks.append(getattr(self.pretrained_model,l).register_forward_hook(self._forward_hook(l)) )\n",
    "        \n",
    "        \n",
    "        self.resize = torch.nn.Upsample(size=(self.M,self.M), scale_factor=None, mode='bilinear', align_corners=False)\n",
    "        \n",
    "        for l in self.FEATURE_LAYERS_ID:\n",
    "            for i in range(self.NUM_CONV_LAYERS):\n",
    "                layer_name = l+'_conv_' + str(i)\n",
    "                if i == 0:\n",
    "                    setattr(self,layer_name,torch.nn.Conv2d(512, self.D,  kernel_size = (1,1), stride=1))\n",
    "                else:\n",
    "                    setattr(self,layer_name,torch.nn.Conv2d(self.D, self.D,  kernel_size = (1,1), stride=1))\n",
    "        self.leaky_relu = torch.nn.LeakyReLU(self.alpha, inplace=True)\n",
    "    def _forward_hook(self,layer_id):\n",
    "        def hook(module,input,output):\n",
    "            self.raw_visual_features[layer_id] = output\n",
    "        return hook\n",
    "\n",
    "    def forward(self,x):\n",
    "        out = self.pretrained_model(x)\n",
    "        for l in self.FEATURE_LAYERS_ID:\n",
    "            x = self.resize(self.raw_visual_features[l])\n",
    "            for i in range(self.NUM_CONV_LAYERS):\n",
    "                layer_name = l+'_conv_' + str(i)\n",
    "                x = getattr(self,layer_name)(x)\n",
    "                x = self.leaky_relu(x)\n",
    "            self.visual_features[l] = x\n",
    "        x = torch.stack([self.visual_features[l] for l in self.visual_features.keys()],1)\n",
    "        x = torch.nn.functional.normalize(x ,p=2,dim=2).permute((0,3,4,1,2))\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "7cfc3eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class TextualFeatures(pl.LightningModule):\n",
    "    \"\"\"\n",
    "    Decoder Object\n",
    "\n",
    "    Note that the forward method iterates through all time steps\n",
    "    It expects an input of [batch]\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, cfg):\n",
    "        \n",
    "        \n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        \n",
    "        self.input_dim = cfg['text']['lstm']['input_dims']\n",
    "        self.hidden_dim = cfg['feature_hidden_dimension']\n",
    "        assert self.hidden_dim%2 == 0\n",
    "        self.lstm_hidden_dim = self.hidden_dim//2\n",
    "        self.num_layers = cfg['text']['lstm']['num_layers']\n",
    "        self.dropout = cfg['text']['lstm']['dropout']\n",
    "        self.relu_alpha = cfg['leaky_relu_alpha']\n",
    "        self.lstm_1 = torch.nn.LSTM(self.input_dim, self.lstm_hidden_dim,\n",
    "                                num_layers=1, dropout=self.dropout, bidirectional=True,batch_first=True,)\n",
    "        self.lstm_2 = torch.nn.LSTM(self.hidden_dim, self.lstm_hidden_dim,\n",
    "                                num_layers=1, dropout=self.dropout, bidirectional=True,batch_first=True,)\n",
    "                                   \n",
    "        self.leaky_relu = torch.nn.LeakyReLU(self.relu_alpha, inplace=True)\n",
    "        self.sentence_fc = torch.nn.Sequential(torch.nn.Linear(self.hidden_dim,self.hidden_dim),self.leaky_relu,torch.nn.Linear(self.hidden_dim,self.hidden_dim),self.leaky_relu)\n",
    "        self.word_fc = torch.nn.Sequential(torch.nn.Linear(self.hidden_dim,self.hidden_dim),self.leaky_relu,torch.nn.Linear(self.hidden_dim,self.hidden_dim),self.leaky_relu)\n",
    "        \n",
    "        self.word_linear_comb = torch.nn.Linear(3,1)\n",
    "        self.sentence_linear_comb = torch.nn.Linear(2,1)\n",
    "    def forward(self, x,seq_len):\n",
    "        \"\"\"\n",
    "       \n",
    "        \"\"\"\n",
    "\n",
    "        packed_x = torch.nn.utils.rnn.pack_padded_sequence(x, seq_len,batch_first=True)\n",
    "        packed_output_1, (hidden_1, cell_1) = self.lstm_1(packed_x)\n",
    "        packed_output_2, (hidden_2, cell_2) = self.lstm_2(packed_output_1)\n",
    "        \n",
    "        output_1, _ = torch.nn.utils.rnn.pad_packed_sequence(packed_output_1,batch_first=True)\n",
    "        output_2, _ = torch.nn.utils.rnn.pad_packed_sequence(packed_output_2,batch_first=True)\n",
    "        \n",
    "        word_feature = self.word_linear_comb(torch.stack([x,output_1,output_2,],-1)).squeeze(-1)\n",
    "        word_feature = self.word_fc(word_feature)\n",
    "        \n",
    "        #start embedding taken from the backward lstm\n",
    "#         sentence_features_backwards = [output_1[:,0,self.hidden_dim:],output_1[:,0,self.hidden_dim:]],-1)\n",
    "        \n",
    "        #end of sentence embedding taken from the forward lstm\n",
    "        sentence_end_1 = []\n",
    "        sentence_end_2 = []\n",
    "        for i,end in enumerate(seq_len):\n",
    "            sentence_end_1.append(output_1[i,end-1,:self.lstm_hidden_dim])\n",
    "            sentence_end_2.append(output_2[i,end-1,:self.lstm_hidden_dim])\n",
    "#         sentence_features_forward = torch.cat([torch.stack(sentence_end_1,0),torch.stack(sentence_end_1,0)],-1)\n",
    "        \n",
    "        first_layer_sentence_feature = torch.cat([torch.stack(sentence_end_1,0),output_1[:,0,self.lstm_hidden_dim:]],-1)\n",
    "        second_layer_sentence_feature = torch.cat([torch.stack(sentence_end_2,0),output_2[:,0,self.lstm_hidden_dim:]],-1)\n",
    "        sentence_feature = self.sentence_linear_comb(torch.stack([first_layer_sentence_feature,second_layer_sentence_feature],-1)).squeeze(-1)\n",
    "        sentence_feature = self.sentence_fc(sentence_feature)\n",
    "        \n",
    "        \n",
    "    \n",
    "        return  word_feature, sentence_feature\n",
    "\n",
    "    def get_len_mask(self, batch_size, max_len, seq_lens):\n",
    "        \"\"\"Generates an upper-triangular matrix of -inf, with zeros on diag.\"\"\"\n",
    "        block = torch.ones(batch_size, max_len, max_len)\n",
    "        for i in range(batch_size):\n",
    "            seq_len = seq_lens[i]\n",
    "            block[i, :seq_len, :seq_len] = torch.zeros(seq_len, seq_len)\n",
    "        return block.bool()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "bf191edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_model = TextualFeatures(cfg['model']).cuda()\n",
    "output,output2= text_model(text,length)\n",
    "viz_model = VisualFeatures()\n",
    "img_output = viz_model(img)\n",
    "# print(cell.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "id": "b6a05825",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MultimodalAttention(pl.LightningModule):\n",
    "    \n",
    "    def __init__(self,cfg):\n",
    "        super().__init__()\n",
    "        self.L = 4\n",
    "        self.M = 18\n",
    "        self.gamma_1 = 5\n",
    "        self.gamma_2 = 10\n",
    " \n",
    "       \n",
    "        \n",
    "    def forward(self,word_feature, sentence_feature, image_feature,seq_lens):\n",
    "        \n",
    "        '''\n",
    "        word_feature dims (B,T,D)\n",
    "        image_feature dims (B,M,M,L,D)\n",
    "        '''\n",
    "        #reshape to (B,M,M,T,L,D)\n",
    "        batch_size = word_feature.shape[0]\n",
    "        word_image_max_score = self.get_pertinence_scores(word_feature,image_feature,batch_size)\n",
    "        sentence_image_score = self.get_pertinence_scores(sentence_feature.unsqueeze(1),image_feature,batch_size).squeeze(2)\n",
    "        \n",
    "        \n",
    "        aggregated_sentence_image_score = torch.exp(word_image_max_score * self.gamma_1) *  self.get_len_mask(batch_size , max_word_len,seq_lens)\n",
    "        aggregated_sentence_image_score = torch.log(torch.sum(aggregated_sentence_image_score,2) ** (1/self.gamma_1))\n",
    "        \n",
    "        return aggregated_sentence_image_score, sentence_image_score \n",
    "\n",
    "    def get_pertinence_scores(self,word_feature,image_feature,batch_size):\n",
    "        # #reshape to (B,B'M,M,T,L,D) repeated along dim 0\n",
    "        reshaped_word_feature = word_feature.unsqueeze(1).unsqueeze(1).unsqueeze(4).unsqueeze(0).expand(batch_size,-1,self.M,self.M,-1,self.L,-1)\n",
    "        max_word_len = reshaped_word_feature.shape[4]\n",
    "        #reshape to (B,B'M,M,T,L,D) repeated along dim 1\n",
    "        reshaped_image_feature = image_feature.unsqueeze(3).unsqueeze(1).expand(-1,batch_size,-1,-1,max_word_len,-1,-1)\n",
    "        #heatmap dims (B,B',M,M,T,L)\n",
    "        similarity_heatmap = F.relu(F.cosine_similarity(reshaped_word_feature,reshaped_image_feature,dim=6))\n",
    "        \n",
    "        # collapse image width and heigh dimensions into single dim for weighted summing via matrix mult\n",
    "        # reshape so that dimension to sum across is at the end  \n",
    "        # (B,B' T, L, 1, MXM)\n",
    "        similarity_heatmap_flat = torch.flatten(similarity_heatmap, start_dim=2, end_dim=3).permute(0,1,3,4,2).unsqueeze(4)\n",
    "        # (B,B',T,L,MXM,D)\n",
    "        # collapse width and height dims for image_feature for weighted summing via matrix mult\n",
    "        image_feature_flat = torch.flatten(image_feature, start_dim=1, end_dim=2).permute(0,2,1,3).unsqueeze(1).unsqueeze(1).expand(-1,batch_size,max_word_len,-1,-1,-1)\n",
    "\n",
    "        visual_word_attention = torch.matmul(similarity_heatmap_flat,image_feature_flat).squeeze(4)\n",
    "        \n",
    "        #(B,B',T,L,D)\n",
    "        visual_word_attention = torch.nn.functional.normalize(visual_word_attention ,p=2,dim=4)\n",
    "        \n",
    "        #(B,B',T,L)\n",
    "        word_image_pertinence_score = F.cosine_similarity(word_feature.unsqueeze(2).unsqueeze(1).expand(-1,batch_size,-1,self.L,-1),visual_word_attention,dim=4)\n",
    "        \n",
    "        #(B,B',T,)\n",
    "        word_image_max_pertinence_score,_ = torch.max(word_image_pertinence_score,dim=3)\n",
    "        return word_image_max_pertinence_score\n",
    "    \n",
    "    def get_len_mask(self, batch_size, max_len, seq_lens):\n",
    "        \"\"\"Generates an 'upper-triangular matrix' with 1 in places without mask\"\"\"\n",
    "        block = torch.zeros(batch_size, max_len)\n",
    "        for i,l in enumerate(seq_lens):\n",
    "            block[i, :l] = torch.ones(1, l)\n",
    "        block = block.unsqueeze(0).expand(batch_size,-1,-1)\n",
    "        return block.cuda()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "id": "edf52e50",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "duplicate argument 'sentence_image_score' in function definition (3968648132.py, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipykernel_835/3968648132.py\"\u001b[0;36m, line \u001b[0;32m3\u001b[0m\n\u001b[0;31m    def get_multimodal_loss(self,sentence_image_score,sentence_image_score):\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m duplicate argument 'sentence_image_score' in function definition\n"
     ]
    }
   ],
   "source": [
    "class VisualGroundingModel(pl.LightningModule):\n",
    "    self.gamma_2 = 10\n",
    "    def get_multimodal_loss(self,sentence_image_score):\n",
    "        \n",
    "        score = torch.exp(sentence_image_score * self.gamma_2)\n",
    "        fixed_image_score_ = score / torch.sum(score,dim=1,keepdim=True)\n",
    "        fixed_sentence_score = score / torch.sum(score,dim=0,keepdim=True)\n",
    "        \n",
    "        loss = -torch.sum(torch.log(torch.diagonal(fixed_image_score, 0)) + torch.log(torch.diagonal(fixed_image_score, 0)))\n",
    "        return loss\n",
    "    \n",
    "    def get_full_multimodal_loss(self,aggregated_sentence_image_score,sentence_image_score):\n",
    "        return self.get_multimodal_loss(aggregated_sentence_image_score) + self.get_multimodal_loss(sentence_image_score)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "id": "3aef5f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "multimodal_attn = MultimodalAttention(cfg)\n",
    "m_output1,m_output2 = multimodal_attn(output.cuda(),output2.cuda(),img_output.cuda(),length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "id": "1724a79b",
   "metadata": {},
   "outputs": [],
   "source": [
    "score = torch.exp(m_output1 * gamma_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "id": "58d0f85d",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = -torch.sum(torch.log(torch.diagonal(fixed_image_score, 0)) + torch.log(torch.diagonal(fixed_image_score, 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "id": "46757a19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(6.5917, device='cuda:0', grad_fn=<NegBackward>)"
      ]
     },
     "execution_count": 329,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "id": "6915533d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_image_score = score / torch.sum(score,dim=1,keepdim=True)\n",
    "fixed_sentence_score = score / torch.sum(score,dim=0,keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "id": "96cb0f0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 1])"
      ]
     },
     "execution_count": 315,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(aggregated_score,dim=1,keepdim=True).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "237ce3ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_feature = output\n",
    "image_feature = img_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "e9a77e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "reshaped_word_feature = word_feature.unsqueeze(1).unsqueeze(1).unsqueeze(4).unsqueeze(0).expand(batch_size,-1,M,M,-1,L,-1).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "c383571b",
   "metadata": {},
   "outputs": [],
   "source": [
    "reshaped_image_feature = image_feature.unsqueeze(3).unsqueeze(1).expand(-1,batch_size,-1,-1,max_word_len,-1,-1).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "631943e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_heatmap = F.relu(F.cosine_similarity(reshaped_word_feature,reshaped_image_feature,dim=6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "9be2a9fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 3, 18, 18, 26, 4, 1024])"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reshaped_image_feature.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "80d898c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size =3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "70b33edd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 3, 26, 4, 324, 1024])"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_feature_flat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "d6dac62f",
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma_1 =5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51441e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregated_sentence_image_score = torch.exp(word_image_max_score * gamma_1) *  self.get_len_mask(batch_size , max_word_len,seq_lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "0f5298b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_heatmap_flat = torch.flatten(similarity_heatmap, start_dim=2, end_dim=3).permute(0,1,3,4,2).unsqueeze(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "5438d380",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_feature_flat = torch.flatten(image_feature, start_dim=1, end_dim=2).permute(0,2,1,3).unsqueeze(1).unsqueeze(1).expand(-1,batch_size,max_word_len,-1,-1,-1).cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "9ae2de20",
   "metadata": {},
   "outputs": [],
   "source": [
    "visual_word_attention = torch.matmul(similarity_heatmap_flat,image_feature_flat).squeeze(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "f63423b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 3, 26, 4, 1024])"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "visual_word_attention.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "id": "3adabfc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_image_pertinence_score = F.cosine_similarity(word_feature.unsqueeze(2).unsqueeze(1).expand(-1,batch_size,-1,L,-1),visual_word_attention,dim=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "id": "0ab0f84a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 3, 26, 4])"
      ]
     },
     "execution_count": 298,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_image_pertinence_score.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6981af1c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
