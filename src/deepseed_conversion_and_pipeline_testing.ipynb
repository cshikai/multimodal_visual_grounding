{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "58b4ef19",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from inference_api.visual_attention.manager import VAManager\n",
    "from inference_api.visual_attention.log_manager import VALogDatabaseManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b8df301",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:allennlp.modules.elmo:Initializing ELMo\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elmo embedder initialized on gpu?: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/elasticsearch/_sync/client/__init__.py:395: SecurityWarning: Connecting to 'https://elasticsearch:9200' using TLS with verify_certs=False is insecure\n",
      "  _transport = transport_class(\n"
     ]
    }
   ],
   "source": [
    "log_manager = VALogDatabaseManager('visual_attention')\n",
    "va_manager = VAManager(log_manager)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7818e53f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/urllib3/connectionpool.py:1045: InsecureRequestWarning: Unverified HTTPS request is being made to host 'elasticsearch'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n",
      "INFO:elastic_transport.transport:GET https://elasticsearch:9200/documents_m2e2/_doc/5c8112081e328a80faa24a2c5de32868 [status:200 duration:0.013s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "int() argument must be a string, a bytes-like object or a number, not 'NoneType'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_5967/2734802106.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mva_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"5c8112081e328a80faa24a2c5de32868\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/src/inference_api/common/inference/manager.py\u001b[0m in \u001b[0;36minfer\u001b[0;34m(self, indexes)\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0mdata_generator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0mdata_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_generator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdata_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/src/inference_api/visual_attention/data_reader.py\u001b[0m in \u001b[0;36mget_generator\u001b[0;34m(self, indexes)\u001b[0m\n\u001b[1;32m     50\u001b[0m                 visual_entities)\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mimage_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_entity_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbounding_box\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobject_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlinked_image\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mimage_generator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_entity_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken_span\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlinked_text\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtext_generator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlinked_image\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlinked_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/src/inference_api/visual_attention/data_reader.py\u001b[0m in \u001b[0;36mget_generator\u001b[0;34m(self, images)\u001b[0m\n\u001b[1;32m     82\u001b[0m             \u001b[0mimage_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m             \u001b[0mPIL_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfromarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muint8\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'RGB'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m             \u001b[0mN\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentity_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: int() argument must be a string, a bytes-like object or a number, not 'NoneType'"
     ]
    }
   ],
   "source": [
    "output = va_manager.infer([\"5c8112081e328a80faa24a2c5de32868\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "088ab9c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.utilities.deepspeed import (convert_zero_checkpoint_to_fp32_state_dict)\n",
    "convert_zero_checkpoint_to_fp32_state_dict('/models/flickr_trained/best.ckpt', '/models/flickr_trained/model.ckpt',tag=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aa96d005",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['apple, banana', 'cherry']\n"
     ]
    }
   ],
   "source": [
    "txt = \"apple, banana, cherry\"\n",
    "\n",
    "# setting the maxsplit parameter to 1, will return a list with 2 elements!\n",
    "x = txt.rsplit(\", \", 1)\n",
    "\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1fa5fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_one = {'name': 'John', 'last_name': 'Doe', 'job': 'Python Consultant'}\n",
    "dict_two = {'name': 'Jane', 'last_name': 'Doe', 'job': 'Community Manager'}\n",
    "batch = [dict_one.items(),dict_two.items()]\n",
    "\n",
    "for a in zip(*batch):\n",
    "    print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "225dc6db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "# from elasticsearch import RequestsHttpConnection\n",
    "\n",
    "from inference_api.common.inference.data_reader import DataReader\n",
    "\n",
    "class VALiveDataReader(DataReader):\n",
    "\n",
    "    ELASTIC_URL = \"https://elasticsearch:9200\"\n",
    "#     FIELDS = ['']\n",
    "    def __init__(self):\n",
    "        self.client = Elasticsearch(self.ELASTIC_URL,\n",
    "                                    basic_auth=('elastic', 'changeme'),\n",
    "                                    verify_certs=False\n",
    "#                                     connection_class=RequestsHttpConnection\n",
    "                                    )\n",
    "\n",
    "\n",
    "    def read(self,document_id):\n",
    "        result = self.client.get(index='documents',\n",
    "                                 id=document_id,\n",
    "#                                  stored_fields =self.FIELDS\n",
    "                                )\n",
    "        return result\n",
    "\n",
    "reader = VALiveDataReader()\n",
    "result = reader.read(\"141dc5e3e4e399c880f5d96f03c07c0d\")\n",
    "text = result['_source']['content']\n",
    "text_content = result['_source']['content']\n",
    "visual_entities = result['_source']['visual_entities']\n",
    "text_entities = result['_source']['text_entities']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d95a8312",
   "metadata": {},
   "outputs": [],
   "source": [
    "visual_entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ca8cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35bbf2e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "bounding_boxes = visual_entities[0]['person_bbox']\n",
    "bounding_boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83264601",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import json\n",
    "visual_entities = result['_source']['visual_entities']\n",
    "\n",
    "# visual_entities\n",
    "\n",
    "image_url = visual_entities[0]['file_name']\n",
    "\n",
    "entity_ids = visual_entities[0]['person_id']\n",
    "bounding_boxes = visual_entities[0]['person_bbox']\n",
    "\n",
    "\n",
    "import requests\n",
    "import numpy as np\n",
    "server_path = image_url\n",
    "body = {'server_path': server_path}\n",
    "r = requests.get(\n",
    "        'http://image_server:8000/download/', json=body)\n",
    "\n",
    "image = np.asarray(r.json()['image'])\n",
    "\n",
    "\n",
    "print(image.shape)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# Display the image\n",
    "ax.imshow(image)\n",
    "\n",
    "# Create a Rectangle patch\n",
    "bounding_box = bounding_boxes[0]\n",
    "top_right_x = max(bounding_box[0],0)\n",
    "top_right_y = max(bounding_box[1],0)\n",
    "\n",
    "delta_x = max(bounding_box[2],0) -top_right_x \n",
    "delta_y = max(bounding_box[3],0) - top_right_y\n",
    "\n",
    "\n",
    "rect = patches.Rectangle((top_right_x, top_right_y), delta_x, delta_y, linewidth=1, edgecolor='r', facecolor='none')\n",
    "\n",
    "# Add the patch to the Axes\n",
    "ax.add_patch(rect)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a611ae3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import json\n",
    "\n",
    "import requests\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# class VisualTextEntityExtractor:\n",
    "from elasticsearch import Elasticsearch\n",
    "\n",
    "\n",
    "from inference_api.common.inference.data_reader import DataReader\n",
    "class VALiveDataReader(DataReader):\n",
    "\n",
    "    ELASTIC_URL = \"https://elasticsearch:9200\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.text_entity_extractor = UnknownTextEntityExtractor()\n",
    "        self.visual_entity_extractor = UnknownVisualEntityExtractor()\n",
    "        self.client = Elasticsearch(self.ELASTIC_URL,\n",
    "                                    basic_auth=('elastic', 'changeme'),\n",
    "                                    verify_certs=False\n",
    "                                    )\n",
    "\n",
    "    def get_generator(self, document_ids):\n",
    "        index = 0\n",
    "        for document_id in document_ids:\n",
    "            result = self.client.get(index='documents',\n",
    "                                     id=document_id,\n",
    "                                     )\n",
    "\n",
    "            text_content = result['_source']['content']\n",
    "            visual_entities = result['_source']['visual_entities']\n",
    "            text_entities = result['_source']['text_entities']\n",
    "\n",
    "            text_generator = self.text_entity_extractor.get_generator(\n",
    "                text_content, text_entities)\n",
    "            image_generator = self.visual_entity_extractor.get_generator(\n",
    "                visual_entities)\n",
    "\n",
    "            for image_url, image_data, bounding_box in image_generator:\n",
    "                for text, token_span in text_generator:\n",
    "                    yield {\n",
    "                        'index': index,\n",
    "                        'image_url': image_url,\n",
    "                        'text': text,\n",
    "                        'image': image_data,\n",
    "                        'token_span': token_span,\n",
    "                        'bounding_box': bounding_box}\n",
    "\n",
    "\n",
    "        \n",
    "            \n",
    "        \n",
    "class UnknownVisualEntityExtractor:\n",
    "    \n",
    "    \n",
    "    def __init__(self):\n",
    "         pass\n",
    "    def get_generator(self,images): \n",
    "        \n",
    "        \n",
    "        for image in images:\n",
    "            entity_ids = image['person_id']\n",
    "            bounding_boxes = image['person_bbox']\n",
    "            image_url = image['file_name']\n",
    "            \n",
    "            image_data = self.download_image(image_url)\n",
    "            N = len(entity_ids)\n",
    "            for i in range(N):\n",
    "                entity_id = entity_ids[i]\n",
    "                if entity_id == -1:\n",
    "                    yield image_url,image_data,bounding_boxes[i]\n",
    "    \n",
    "    def download_image(self,server_path):\n",
    "        body = {'server_path': server_path}\n",
    "        r = requests.get('http://image_server:8000/download/', json=body)\n",
    "        image = np.asarray(r.json()['image'])\n",
    "        return image\n",
    " \n",
    "class UnknownTextEntityExtractor:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.token_mapper = TextTokenMapper()\n",
    "        \n",
    "    def get_generator(self,text,text_entities):\n",
    "        N = len(text_entities)\n",
    "        sentences = self.token_mapper.split_sentences(text)\n",
    "        for i in range(N):\n",
    "            entity = text_entities[i]\n",
    "            if entity['entity_link'] == -1:\n",
    "                sentence_index,span_start,span_end = entity['mention_span']\n",
    "                sentence = sentences[sentence_index]\n",
    "                token_span = self.token_mapper.get_tokens(sentence,entity['mention'],span_start,span_end)\n",
    "                yield sentences[sentence_index],token_span\n",
    "                \n",
    "\n",
    "from spacy.lang.en import English\n",
    "\n",
    "class TextTokenMapper:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.sentencizer = English()\n",
    "        self.sentencizer.add_pipe('sentencizer')\n",
    "    \n",
    "    def split_sentences(self,text):\n",
    "        paras = text.split(\"\\n\")\n",
    "        sentences = []\n",
    "        for para in paras:\n",
    "            str_sents = list(self.sentencizer(para).sents)\n",
    "            for sent in str_sents:\n",
    "                tokens = list(self.sentencizer.tokenizer(sent.text))\n",
    "                tokens = [token.text for token in tokens]\n",
    "                if len(tokens) > 0:\n",
    "                    sentences.append(sent.text)\n",
    "        return sentences\n",
    "    \n",
    "    def get_tokens(self,sentence,entity_mention,span_start,span_end):\n",
    "        tokens = sentence.split(' ')\n",
    "        N = len(tokens)\n",
    "        entity_tokens = len(entity_mention.split(' '))\n",
    "        cumilative_char_index = 0\n",
    "        char_to_token_span = []\n",
    "\n",
    "        for token_index, token in enumerate(tokens):\n",
    "            end_char_index = cumilative_char_index + len(token)\n",
    "            char_to_token_span.append((cumilative_char_index,end_char_index,token_index))\n",
    "            cumilative_char_index = end_char_index + 1                \n",
    "        left = 0\n",
    "        char_2_token_spans = {}\n",
    "        for right in range(entity_tokens-1,N):\n",
    "            left_start_index, left_end_index, left_token_index = char_to_token_span[left]\n",
    "            right_start_index,right_end_index, right_token_index = char_to_token_span[right]\n",
    "            char_2_token_spans[(left_start_index,right_end_index)] = (left_token_index,right_token_index) \n",
    "            left += 1\n",
    "\n",
    "\n",
    "        while span_start > 0 and sentence[span_start - 1] != ' ':\n",
    "            span_start -= 1\n",
    "        while span_end < len(sentence) and sentence[span_end] != ' ':\n",
    "            span_end += 1\n",
    "       \n",
    "        \n",
    "        token_span = char_2_token_spans[(span_start,span_end)]\n",
    "        \n",
    "\n",
    "        return token_span\n",
    "    \n",
    "    \n",
    "# text_ext = UnknownTextEntityExtractor()\n",
    "# for i in text_ext.get_generator(text,text_entities):\n",
    "#     print(i[0])\n",
    "\n",
    "\n",
    "\n",
    "reader = VALiveDataReader()\n",
    "generator = reader.get_generator([\"141dc5e3e4e399c880f5d96f03c07c0d\"])\n",
    "\n",
    "\n",
    "for data in generator:\n",
    "    print(data)\n",
    "    break\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ee9051",
   "metadata": {},
   "outputs": [],
   "source": [
    "'/.a'.isalnum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0520e6e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "visual_entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c4f760",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_entities = result['_source']['text_entities']\n",
    "i= 13\n",
    "entity = text_entities[i]\n",
    "# span = entity['mention_span']\n",
    "# mention_text = entity['mention']\n",
    "text = result['_source']['content']\n",
    "sentence_index,span_start,span_end = entity['sentence_char_span']\n",
    "\n",
    "\n",
    "\n",
    "text = result['_source']['content']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c829721",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "token_mapper = TextTokenMapper()\n",
    "\n",
    "sentences = token_mapper.split_sentences(text)\n",
    "\n",
    "token_mapper.get_tokens(sentences,entity['mention'],entity['sentence_char_span'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebbdbaa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_entities = result['_source']['text_entities']\n",
    "text_entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be0cf37f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "content = result['_source']['content']\n",
    "re.split(' |\\n', content)\n",
    "\n",
    "# span"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5153854f",
   "metadata": {},
   "outputs": [],
   "source": [
    "result['_source']['content'].split(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba03630",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4278428f",
   "metadata": {},
   "outputs": [],
   "source": [
    "result['_source']['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80208557",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
