{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa4e38e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Pytorch pre-release version 1.11.0a0+bfe5ad2 - assuming intent to test it\n"
     ]
    }
   ],
   "source": [
    "from config.config import cfg\n",
    "from model.model import VisualGroundingModel\n",
    "from data.dataset import VisualGroundingDataset\n",
    "from data.preprocessor import PreProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f25a608",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from config.config import cfg\n",
    "import torch\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6409ce9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "83ea1194",
   "metadata": {},
   "outputs": [],
   "source": [
    "vgmodel = VisualGroundingModel(cfg, cfg['training']['n_gpu']>1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "54d9912a",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = PreProcessor(cfg)\n",
    "train_dataset = VisualGroundingDataset('train', preprocessor)\n",
    "valid_dataset = VisualGroundingDataset('valid', preprocessor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a9669613",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, collate_fn=train_dataset.preprocessor.collate,\n",
    "                                       batch_size=cfg['training']['batch_size'], shuffle=False, num_workers=cfg['training']['num_workers'])\n",
    "valid_loader = DataLoader(valid_dataset, collate_fn=valid_dataset.preprocessor.collate,\n",
    "                                       batch_size=cfg['training']['batch_size'], shuffle=False, num_workers=cfg['training']['num_workers'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "13eb5970",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VisualGroundingModel(\n",
       "  (text_features): TextualFeatures(\n",
       "    (lstm_1): LSTM(1024, 512, batch_first=True, bidirectional=True)\n",
       "    (lstm_2): LSTM(1024, 512, batch_first=True, bidirectional=True)\n",
       "    (leaky_relu): LeakyReLU(negative_slope=0.25)\n",
       "    (sentence_fc): Sequential(\n",
       "      (0): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      (1): LeakyReLU(negative_slope=0.25)\n",
       "      (2): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      (3): LeakyReLU(negative_slope=0.25)\n",
       "    )\n",
       "    (word_fc): Sequential(\n",
       "      (0): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      (1): LeakyReLU(negative_slope=0.25)\n",
       "      (2): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      (3): LeakyReLU(negative_slope=0.25)\n",
       "    )\n",
       "    (word_linear_comb): Linear(in_features=3, out_features=1, bias=False)\n",
       "    (sentence_linear_comb): Linear(in_features=2, out_features=1, bias=False)\n",
       "  )\n",
       "  (visual_features): VisualFeatures(\n",
       "    (0_conv_0): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (0_conv_1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (0_conv_2): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (1_conv_0): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (1_conv_1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (1_conv_2): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (2_conv_0): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (2_conv_1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (2_conv_2): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (3_conv_0): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (3_conv_1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (3_conv_2): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (leaky_relu): LeakyReLU(negative_slope=0.25)\n",
       "  )\n",
       "  (attention): MultimodalAttention()\n",
       "  (loss): MultimodalLoss()\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vgmodel.to('cuda')\n",
    "vgmodel.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2e5fddd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 4, 256, 18, 18])\n",
      "torch.Size([8, 19, 1024])\n",
      "tensor([[19],\n",
      "        [17],\n",
      "        [16],\n",
      "        [16],\n",
      "        [16],\n",
      "        [14],\n",
      "        [14],\n",
      "        [11]])\n"
     ]
    }
   ],
   "source": [
    "for i in train_loader:\n",
    "#     print(i)\n",
    "    print(i[0].shape)\n",
    "    print(i[1].shape)\n",
    "    print(i[-1])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4f7dcf66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 256, 18, 18])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i[0][0].shape # First item in batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e344baf3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ac3ac26a",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = vgmodel(i[0], i[1], i[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9ae48095",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "49268803",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 8, 18, 18, 1, 1])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[1][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c7e64bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import torchvision.transforms as T\n",
    "transform = T.ToPILImage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bc758296",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "pic should be 2/3 dimensional. Got 6 dimensions.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1057/1452244899.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, pic)\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m         \"\"\"\n\u001b[0;32m--> 218\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_pil_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torchvision/transforms/functional.py\u001b[0m in \u001b[0;36mto_pil_image\u001b[0;34m(pic, mode)\u001b[0m\n\u001b[1;32m    231\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndimension\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 233\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"pic should be 2/3 dimensional. Got {pic.ndimension()} dimensions.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mpic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndimension\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: pic should be 2/3 dimensional. Got 6 dimensions."
     ]
    }
   ],
   "source": [
    "img = transform(output[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d0ae1476",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4c9b6926",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "trainer = pl.Trainer(\n",
    "            devices=cfg['training']['n_gpu'],\n",
    "            accelerator=\"gpu\",\n",
    "            # strategy=self.cfg['training']['accelerator'] if self.distributed else None,\n",
    "            strategy=None,\n",
    "            accumulate_grad_batches=cfg['training']['accumulate_grad_batches'],\n",
    "            max_epochs=cfg['training']['epochs'],\n",
    "            default_root_dir=cfg['training']['local_trained_model_path'],\n",
    "            log_every_n_steps=cfg['training']['log_every_n_steps']\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b756384",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Missing logger folder: /models/trained_models_resnet/lightning_logs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "\n",
      "  | Name            | Type                | Params\n",
      "--------------------------------------------------------\n",
      "0 | text_features   | TextualFeatures     | 16.8 M\n",
      "1 | visual_features | VisualFeatures      | 9.4 M \n",
      "2 | attention       | MultimodalAttention | 0     \n",
      "3 | loss            | MultimodalLoss      | 0     \n",
      "--------------------------------------------------------\n",
      "26.2 M    Trainable params\n",
      "0         Non-trainable params\n",
      "26.2 M    Total params\n",
      "104.989   Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Sanity Checking: 0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 40 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Sanity Checking DataLoader 0:   0%|                                                                                                                                    | 0/2 [00:03<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/pytorch_lightning/utilities/data.py:72: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 8. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "  warning_cache.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                             \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 40 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|                                                                                                                    | 5/156261 [00:17<148:08:56,  3.41s/it, loss=66.3, v_num=0]"
     ]
    }
   ],
   "source": [
    "trainer.fit(vgmodel, train_loader, valid_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7acec801",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
