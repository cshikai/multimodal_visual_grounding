{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3d12bb61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/manifest/flickr/train_manifest.csv\n",
      "max val 78\n",
      "['Four young adults sit outside on a wooden deck near a building around a small round table , while another person stands on the edge of the deck , leaning on the wooden railing , with the sun shining on one of them and the rest of them are in the shade , two of them crossing their legs , one holding a cellphone out in front of himself and another holding a green and red soda can.']\n",
      "99158 out of 127130 is 30 len or below. thats 0.7799732557224888 percent\n",
      "=========\n",
      "\n",
      "/data/manifest/mscoco/train_manifest.csv\n",
      "max val 179\n",
      "['The intricacies of the motorcycle are                                                                                                                                                                      brought to life  by the sepia photography.']\n",
      "399981 out of 414113 is 30 len or below. thats 0.9658740488707188 percent\n",
      "=========\n",
      "\n",
      "/data/manifest/visualgenome/train_manifest.csv\n",
      "max val 95\n",
      "['Two                                                                                           three police on horses.']\n",
      "432123 out of 432305 is 30 len or below. thats 0.9995790009368386 percent\n",
      "=========\n",
      "\n",
      "/data/manifest/flickr/valid_manifest.csv\n",
      "max val 81\n",
      "['A man wearing a helmet , red pants with white stripes going down the sides and a white and red shirt is on a small bicycle using only his hands while his legs are up in the air , while another man wearing a light blue shirt with dark blue trim and black pants with red stripes going up the sides is standing nearby , gesturing toward the first man and holding a small figurine of one of the seven dwarves.']\n",
      "24936 out of 31785 is 30 len or below. thats 0.7845210004719207 percent\n",
      "=========\n",
      "\n",
      "/data/manifest/mscoco/valid_manifest.csv\n",
      "max val 50\n",
      "['A green surface with  a fifties car model on a plate, topped with a hotdog roll and fries, a napkin dispenser, menus and condiments, has  two sets of  hands on either side, reaching for items, one of which is part of a black and  red/wearing woman.'\n",
      " 'A large square concrete wall, which shows people over the rim, has inside of it a pinkish cloud of dust and a fallen bull as well as two horses, the closest of which is trotting forward with a man in western gear , who is looking back at the bull.']\n",
      "195879 out of 202654 is 30 len or below. thats 0.9665686342238495 percent\n",
      "=========\n",
      "\n",
      "/data/manifest/visualgenome/valid_manifest.csv\n",
      "max val 33\n",
      "[\"Very fabulous building, the peace tower: gothic, clever, cleverly mad, full of memorials, appointments, fixtures, '370 gargoyles, grotesques, and friezes', none of which are, sadly, visible here. it's still wonderful, look it up.\"]\n",
      "108027 out of 108078 is 30 len or below. thats 0.9995281185810249 percent\n",
      "=========\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying (Retry(total=237, connect=237, read=240, redirect=240, status=240)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f032ee0cb80>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')': /v2.13/tasks.get_all\n",
      "Retrying (Retry(total=236, connect=236, read=240, redirect=240, status=240)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f032ef48850>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')': /v2.13/tasks.get_all\n",
      "Retrying (Retry(total=235, connect=235, read=240, redirect=240, status=240)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f032ef484c0>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')': /v2.13/tasks.get_all\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "CAP = 15\n",
    "# CAP DATA AT 20 OR 15.\n",
    "\n",
    "\n",
    "\n",
    "def word_count(string):\n",
    "    return len(string.split(\" \"))\n",
    "\n",
    "for i in ['train','valid']:\n",
    "    for j in ['flickr','mscoco','visualgenome']:\n",
    "        f = '/data/manifest/{}/{}_manifest.csv'.format(j,i)\n",
    "        print(f)\n",
    "        df = pd.read_csv(f)\n",
    "        df['wordcount'] = df.caption.apply(word_count)\n",
    "        print('max val', df.wordcount.max())\n",
    "        print(df[df.wordcount==df.wordcount.max()]['caption'].values)\n",
    "        \n",
    "        cramp_df = df[df.wordcount<=CAP]\n",
    "        print('{} out of {} is 30 len or below. thats {} percent'.format(len(cramp_df),len(df),len(cramp_df)/len(df)))\n",
    "        print('=========\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "87bc1467",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.0000, 1.0000],\n",
      "         [0.5547, 0.8321]],\n",
      "\n",
      "        [[0.6247, 0.7809],\n",
      "         [0.6508, 0.7593]]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[True, True],\n",
       "         [True, True]],\n",
       "\n",
       "        [[True, True],\n",
       "         [True, True]]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "a = torch.arange(8).reshape((2,2,2)).float()\n",
    "\n",
    "b = torch.nn.functional.normalize(a,p=2,dim=-1)\n",
    "print(b)\n",
    "\n",
    "c = []\n",
    "for i in range((a.shape[1])):\n",
    "    d = torch.nn.functional.normalize(a[:,i:i+1,:],p=2,dim=-1)\n",
    "    c.append(d)\n",
    "    \n",
    "c = torch.cat(c,dim=1)\n",
    "c==b\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3d624f61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.5, 0.5, 0.5, 0.5]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "learning_rate = 2\n",
    "model = torch.nn.Linear(2, 1)\n",
    "def get_learning_rate(epoch):\n",
    "    if epoch < 10:\n",
    "        return 1\n",
    "    elif epoch < 15:\n",
    "        return 0.5\n",
    "    else:\n",
    "        return 0.25\n",
    "    \n",
    "optimizer = torch.optim.Adam(\n",
    "                model.parameters(), lr=learning_rate)\n",
    "            \n",
    "    \n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(\n",
    "                optimizer, lr_lambda=get_learning_rate)\n",
    "\n",
    "\n",
    "\n",
    "lrs = []\n",
    "\n",
    "for i in range(20):\n",
    "    optimizer.step()\n",
    "    lrs.append(optimizer.param_groups[0][\"lr\"])\n",
    "#     print(\"Factor = \", round(0.65 ** i,3),\" , Learning Rate = \",round(optimizer.param_groups[0][\"lr\"],3))\n",
    "    scheduler.step()\n",
    "    \n",
    "print(lrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5366f55f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "import os\n",
    "root_folder = '/data/image_manifest/flickr/train'\n",
    "data = dd.read_parquet(os.path.join(root_folder),\n",
    "                                    \n",
    "                                    )  # this is lazy loading, its not actually loading into memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ceda8e10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/parquet/flickr/train\n"
     ]
    }
   ],
   "source": [
    "from data.text_tensor.dataset import VGTextDataset\n",
    "from config.config import cfg\n",
    "train_dataset = VGTextDataset('train', cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "aa1771d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[259,  85, 120, 112, 260, 261, 261, 261, 261, 261, 261, 261, 261, 261,\n",
       "          261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261,\n",
       "          261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261,\n",
       "          261, 261, 261, 261, 261, 261, 261, 261],\n",
       "         [259, 122, 112, 118, 111, 104, 260, 261, 261, 261, 261, 261, 261, 261,\n",
       "          261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261,\n",
       "          261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261,\n",
       "          261, 261, 261, 261, 261, 261, 261, 261],\n",
       "         [259, 104, 118, 122, 116, 260, 261, 261, 261, 261, 261, 261, 261, 261,\n",
       "          261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261,\n",
       "          261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261,\n",
       "          261, 261, 261, 261, 261, 261, 261, 261],\n",
       "         [259, 120, 106, 117, 105, 260, 261, 261, 261, 261, 261, 261, 261, 261,\n",
       "          261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261,\n",
       "          261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261,\n",
       "          261, 261, 261, 261, 261, 261, 261, 261],\n",
       "         [259, 116, 105,  98, 104, 104, 122, 260, 261, 261, 261, 261, 261, 261,\n",
       "          261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261,\n",
       "          261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261,\n",
       "          261, 261, 261, 261, 261, 261, 261, 261],\n",
       "         [259, 105,  98, 106, 115, 260, 261, 261, 261, 261, 261, 261, 261, 261,\n",
       "          261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261,\n",
       "          261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261,\n",
       "          261, 261, 261, 261, 261, 261, 261, 261],\n",
       "         [259, 109, 112, 112, 108, 260, 261, 261, 261, 261, 261, 261, 261, 261,\n",
       "          261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261,\n",
       "          261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261,\n",
       "          261, 261, 261, 261, 261, 261, 261, 261],\n",
       "         [259,  98, 117, 260, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261,\n",
       "          261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261,\n",
       "          261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261,\n",
       "          261, 261, 261, 261, 261, 261, 261, 261],\n",
       "         [259, 117, 105, 102, 106, 115, 260, 261, 261, 261, 261, 261, 261, 261,\n",
       "          261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261,\n",
       "          261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261,\n",
       "          261, 261, 261, 261, 261, 261, 261, 261],\n",
       "         [259, 105,  98, 111, 101, 116, 260, 261, 261, 261, 261, 261, 261, 261,\n",
       "          261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261,\n",
       "          261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261,\n",
       "          261, 261, 261, 261, 261, 261, 261, 261],\n",
       "         [259, 120, 105, 106, 109, 102, 260, 261, 261, 261, 261, 261, 261, 261,\n",
       "          261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261,\n",
       "          261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261,\n",
       "          261, 261, 261, 261, 261, 261, 261, 261],\n",
       "         [259, 105,  98, 111, 104, 106, 111, 104, 260, 261, 261, 261, 261, 261,\n",
       "          261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261,\n",
       "          261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261,\n",
       "          261, 261, 261, 261, 261, 261, 261, 261],\n",
       "         [259, 112, 118, 117, 260, 261, 261, 261, 261, 261, 261, 261, 261, 261,\n",
       "          261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261,\n",
       "          261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261,\n",
       "          261, 261, 261, 261, 261, 261, 261, 261],\n",
       "         [259, 106, 111, 260, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261,\n",
       "          261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261,\n",
       "          261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261,\n",
       "          261, 261, 261, 261, 261, 261, 261, 261],\n",
       "         [259, 117, 105, 102, 260, 261, 261, 261, 261, 261, 261, 261, 261, 261,\n",
       "          261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261,\n",
       "          261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261,\n",
       "          261, 261, 261, 261, 261, 261, 261, 261],\n",
       "         [259, 122,  98, 115, 101,  47, 260, 261, 261, 261, 261, 261, 261, 261,\n",
       "          261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261,\n",
       "          261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261,\n",
       "          261, 261, 261, 261, 261, 261, 261, 261]]),\n",
       " 0,\n",
       " 16)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f56a666a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Four young adults sit outside on a wooden deck near a building around a small round table , while another person stands on the edge of the deck , leaning on the wooden railing , with the sun shining on one of them and the rest of them are in the shade , two of them crossing their legs , one holding a cellphone out in front of himself and another holding a green and red soda can.'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c63b9305",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-04-05 14:06:16,732 - clearml.storage - INFO - Downloading: 5.00MB / 11.55MB @ 11.51MBs from s3://experiment-logging/multimodal/datasets%2Fmultimodal/mscoco_manifest.71a397b04b5d476bb5543306269e28af/artifacts/data/dataset.71a397b04b5d476bb5543306269e28af.zip\n",
      "2022-04-05 14:06:17,048 - clearml.storage - INFO - Downloading: 10.05MB / 11.55MB @ 15.93MBs from s3://experiment-logging/multimodal/datasets%2Fmultimodal/mscoco_manifest.71a397b04b5d476bb5543306269e28af/artifacts/data/dataset.71a397b04b5d476bb5543306269e28af.zip\n",
      "2022-04-05 14:06:17,139 - clearml.storage - INFO - Downloaded 11.55 MB successfully from s3://experiment-logging/multimodal/datasets%2Fmultimodal/mscoco_manifest.71a397b04b5d476bb5543306269e28af/artifacts/data/dataset.71a397b04b5d476bb5543306269e28af.zip , saved to /root/.clearml/cache/storage_manager/datasets/8da9728d393f1f5a5076e2defd9b9e36.dataset.71a397b04b5d476bb5543306269e28af.zip\n",
      "['/tmp/71a397b04b5d476bb5543306269e28af/train_manifest.csv']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "from clearml import Dataset, Task\n",
    "\n",
    "DATA_PROJECT_NAME = \"datasets/multimodal\"\n",
    "\n",
    "\n",
    "TASK_NAME = \"visual_grounding_training_data_generation\"\n",
    "\n",
    "DATASET_ROOT = '/data/processed/'\n",
    "\n",
    "OUTPUT_URL = \"s3://experiment-logging/multimodal\"\n",
    "\n",
    "\n",
    "input_train_dataset_paths = []\n",
    "input_valid_dataset_paths = []\n",
    "\n",
    "args = {\n",
    "\n",
    "    'num_captions': 5,\n",
    "    'npartitions': 100,\n",
    "    'batch_size': 4,\n",
    "    'input_manifests': ['71a397b04b5d476bb5543306269e28af', ],\n",
    "    'dataset_name': \"VisualGround_FLICKR_manifest_100paritions_4_batch\"\n",
    "}\n",
    "\n",
    "\n",
    "TEMP_PATH = '/tmp'\n",
    "if not os.path.exists(TEMP_PATH):\n",
    "    os.makedirs(TEMP_PATH)\n",
    "for dataset_id in args['input_manifests']:\n",
    "    dataset = Dataset.get(dataset_id=dataset_id)\n",
    "    local_root_path = dataset.get_mutable_local_copy(\n",
    "        os.path.join(TEMP_PATH, dataset_id))\n",
    "\n",
    "    input_train_dataset_paths.append(os.path.join(\n",
    "        local_root_path, 'train_manifest.csv'))\n",
    "\n",
    "    valid_path = os.path.join(\n",
    "        local_root_path, 'valid_manifest.csv')\n",
    "\n",
    "    if os.path.exists(valid_path):\n",
    "        input_valid_dataset_paths.append(valid_path)\n",
    "print(input_train_dataset_paths)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a26058",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
